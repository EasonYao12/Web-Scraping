{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 爬蟲 momo購物網 post 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"http://www.momoshop.com.tw/search/\"\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4)'\n",
    "           ' AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1 '\n",
    "           'Safari/605.1.15'}\n",
    "\n",
    "r = requests.get(URL +\"searchShop.jsp?keyword=iPhone\", headers=headers)\n",
    "\n",
    "print(r.status_code)  # 200 means success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT Soft_Job 文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bbs/Soft_Job/M.1554988690.A.903.html\n",
      "[請益] 該如何證明自己演算法的能力？\n",
      "iven00000000\n",
      "/bbs/Soft_Job/M.1554989603.A.6B9.html\n",
      "[請益] 筆記軟體推薦\n",
      "PttZF\n",
      "/bbs/Soft_Job/M.1554998165.A.8EB.html\n",
      "[請益] 未來發展與薪水抉擇\n",
      "kay0629\n",
      "/bbs/Soft_Job/M.1501827536.A.DF2.html\n",
      "[公告] 本板板規  2017/4/10更新\n",
      "MOONY135\n",
      "/bbs/Soft_Job/M.1501827692.A.18D.html\n",
      "[公告] 徵才不符板規或徵才自刪公司\n",
      "MOONY135\n",
      "/bbs/Soft_Job/M.1501847358.A.F41.html\n",
      "[情報] 訓練課程與付費APP與網站分享\n",
      "MOONY135\n",
      "/bbs/Soft_Job/M.1501847445.A.045.html\n",
      "[情報] 社群活動與免費APP與網站分享\n",
      "MOONY135\n",
      "/bbs/Soft_Job/M.1498710085.A.78A.html\n",
      "[板務] 請板友提供板務建議\n",
      "s89227\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.ptt.cc/bbs/Soft_Job/index.html\"\n",
    "\n",
    "r = requests.get(URL)\n",
    "if r.status_code == requests.codes.ok: # 200\n",
    "    r.encoding = \"utf8\"\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    tag_divs = soup.find_all(\"div\", class_=\"r-ent\")\n",
    "    for tag in tag_divs:\n",
    "        tag_a = tag.find(\"a\")\n",
    "        print(tag_a[\"href\"])\n",
    "        print(tag_a.text)\n",
    "        print(tag.find(\"div\", class_=\"author\").string)\n",
    "else:\n",
    "    print(\"HTTP failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬股票資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抓取: 3711 網路資料中...\n",
      "等待5秒鐘...\n",
      "抓取: 2330 網路資料中...\n",
      "等待5秒鐘...\n",
      "抓取: 2454 網路資料中...\n",
      "等待5秒鐘...\n",
      "['代碼', '名稱', '狀態', '股價', '昨收', '張數', '最高', '最低']\n",
      "['3711', '日月光投控', '成交', '71.4', '71.8', '4,781', '71.8', '70.7']\n",
      "['2330', '台積電', '成交', '252.0', '254.0', '24,870', '254.0', '251.5']\n",
      "['2454', '聯發科', '成交', '291.5', '300.0', '12,161', '303.0', '291.0']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#目標網址\n",
    "URL = \"https://tw.stock.yahoo.com/q/q?s=\" #後面加要的股票數字代碼\n",
    "\n",
    "\n",
    "\n",
    "def generate_urls(url, stocks):\n",
    "    urls = []\n",
    "    for stock in stocks:\n",
    "        urls.append(url + stock)\n",
    "    return urls\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    stocks = [[\"代碼\", \"名稱\", \"狀態\", \"股價\", \"昨收\", \"張數\", \"最高\", \"最低\"]]\n",
    "    \n",
    "    for url in urls:\n",
    "        stock_id = url.split(\"=\")[-1]\n",
    "        print(stock_id)\n",
    "        print('抓取： '+ stock_id + \" 網路資料中.....\")\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            stock = get_stock(soup, stock_id)\n",
    "            stocks.append(stock)\n",
    "            print(\"wait 5 second....\")\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(\"HTTP failed ...\")\n",
    "    return stocks\n",
    "\n",
    "def get_resource(url):\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4)'\n",
    "               'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1 Safari/605.1.15'}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "def get_stock(soup, stock_id):\n",
    "    table = soup.find_all(text=\"成交\")[0].parent.parent.parent\n",
    "    status = table.select(\"tr\")[0].select(\"th\")[2].text\n",
    "    name =   table.select(\"tr\")[1].select(\"td\")[0].text\n",
    "    price =  table.select(\"tr\")[1].select(\"td\")[2].text\n",
    "    yclose = table.select(\"tr\")[1].select(\"td\")[7].text\n",
    "    volume = table.select(\"tr\")[1].select(\"td\")[6].text\n",
    "    high =   table.select(\"tr\")[1].select(\"td\")[9].text\n",
    "    low  =   table.select(\"tr\")[1].select(\"td\")[10].text\n",
    "    return [stock_id, name[4:-6], status, price, yclose, volume, high, low]\n",
    "\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    stocks = [[\"代碼\",\"名稱\",\"狀態\",\"股價\",\"昨收\",\"張數\",\"最高\",\"最低\"]]\n",
    "    \n",
    "    for url in urls:\n",
    "        stock_id = url.split(\"=\")[-1]\n",
    "        print(\"抓取: \" + stock_id + \" 網路資料中...\")\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            stock = get_stock(soup, stock_id)\n",
    "            stocks.append(stock)\n",
    "            print(\"等待5秒鐘...\")\n",
    "            time.sleep(5) \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")       \n",
    "\n",
    "    return stocks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = generate_urls(URL, [\"3711\", \"2330\", \"2454\"])\n",
    "    # print(urls)\n",
    "    stocks = web_scraping_bot(urls)\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "    save_to_csv(stocks, \"stocks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬Yahoo本週電影新片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抓取: 第1頁 網路資料中...\n",
      "等待5秒鐘...\n",
      "抓取: 第2頁 網路資料中...\n",
      "等待5秒鐘...\n",
      "['中文片名', '英文片名', '期待度', '海報圖片', '上映日']\n",
      "['地獄怪客：血后的崛起', 'Hellboy', '95%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/November2018/Zj24hpguXm3NITIBjEgI-840x1200.jpg', '2019-04-11']\n",
      "['禁入墳場', 'Pet Sematary', '90%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/February2019/zkaR7xCgJ9CNoJfyLvMI-1012x1500.JPG', '2019-04-12']\n",
      "['人盡皆知', 'Everybody Knows', '68%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/February2019/4FYsUy16Xf9B6Uonmo2r-1984x2835.JPG', '2019-04-12']\n",
      "['女王的柯基', 'The Queen’s Corgi', '76%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/April2019/HrLGWCSZ005b67JzSM9N-2500x3360.png', '2019-04-12']\n",
      "['十二個想死的少年', '12 Suicidal Teens', '71%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2019/fLfIHQJO2uHWa3tYckBE-2487x3543.jpg', '2019-04-12']\n",
      "['電競殺手', 'Censor', '68%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2019/m0n84JkfjX6CpqydexAp-1246x1781.jpg', '2019-04-12']\n",
      "['灰影地帶', 'Ashes in the Snow', '73%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/February2019/eeMNnjNLjAt8hVqOvMmy-850x1215.jpg', '2019-04-12']\n",
      "['我們的青春，在台灣', 'Our Youth in Taiwan', '49%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2019/5dZSspv74IkcAxsLsIuB-1500x2172.jpg', '2019-04-12']\n",
      "['一念', 'A Decision', '54%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2019/8bGYeboNRj2QoMIB2V1u-1569x2244.jpg', '2019-04-12']\n",
      "['惡孕', 'Malicious', '86%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2019/wKn0FXKrUHgJZ69zyeCE-1984x2834.jpg', '2019-04-12']\n",
      "['真愛，再出發', 'Departures', '96%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2019/wkOaUb0KVFh1LaKlxoFR-1772x2589.jpg', '2019-04-12']\n",
      "['異類占領', 'Captive State', '45%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/April2019/Fz7RHLDwJqbLI32WvcYL-702x1000.jpg', '2019-04-12']\n",
      "['沒有你的生日', 'Birthday', '77%', 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2019/bXLV274rGqp5weiHEkkJ-1984x2835.jpg', '2019-04-12']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#目標網址\n",
    "URL = \"https://movies.yahoo.com.tw/movie_thisweek.html?page={0}\"\n",
    "\n",
    "def generate_urls(url, start_page, end_page):\n",
    "    urls = []\n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(page))\n",
    "    return urls\n",
    "\n",
    "def get_resource(url):\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4)'\n",
    "               'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1 Safari/605.1.15'}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "def format_date(date_str):\n",
    "    # 取出上映日期\n",
    "    pattern = '\\d+-\\d+-\\d+'\n",
    "    match = re.search(pattern, date_str)\n",
    "    if match is None:\n",
    "        return date_str\n",
    "    else:\n",
    "        return match.group(0)\n",
    "\n",
    "def get_movies(soup):\n",
    "    movies = []\n",
    "    rows = soup.find_all(\"div\", class_=\"release_info_text\")\n",
    "    for row in rows:\n",
    "        movie_name_div = row.find(\"div\", class_=\"release_movie_name\")\n",
    "        cht_name = movie_name_div.a.text.strip()\n",
    "        eng_name = movie_name_div.find(\"div\", class_=\"en\").a.text.strip()\n",
    "        expectation = row.find(\"div\", class_=\"leveltext\").span.text.strip()\n",
    "        photo = row.parent.find_previous_sibling(\n",
    "                \"div\", class_=\"release_foto\")\n",
    "        poster_url = photo.a.img[\"src\"]\n",
    "        release_date = format_date(row.find('div', 'release_movie_time').text)\n",
    "        \n",
    "        movie= [cht_name,eng_name,expectation,\n",
    "                poster_url,release_date]\n",
    "        movies.append(movie)\n",
    "    return movies\n",
    "\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    all_movies = [[\"中文片名\",\"英文片名\",\"期待度\",\"海報圖片\",\"上映日\"]]\n",
    "    page = 1\n",
    "    \n",
    "    for url in urls:\n",
    "        print(\"抓取: 第\" + str(page) + \"頁 網路資料中...\")\n",
    "        page = page + 1\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            movies = get_movies(soup)\n",
    "            all_movies = all_movies + movies\n",
    "            print(\"等待5秒鐘...\")\n",
    "            if soup.find(\"li\", class_=\"nexttxt disabled\"):\n",
    "                break   # 已經沒有下一頁\n",
    "            time.sleep(5) \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")\n",
    "\n",
    "    return all_movies\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = generate_urls(URL, 1, 5)\n",
    "    # print(urls)\n",
    "    movies = web_scraping_bot(urls)\n",
    "    for movie in movies:\n",
    "        print(movie)\n",
    "    save_to_csv(movies, \"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
